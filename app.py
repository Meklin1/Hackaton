"""
Streamlit Exoplanet Classification App
- Upload CSV file with exoplanet data
- Send to FastAPI backend for classification
- Display results with exoplanet_status as first column
- Validate and edit headers if incorrect
"""

import streamlit as st
import pandas as pd
import requests
import io
from typing import Dict, List

# ---------------------------
# Configuration
# ---------------------------
st.set_page_config(
    page_title="Exoplanet Classifier",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for professional appearance
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        background: linear-gradient(120deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 0.5rem;
    }
    .subtitle {
        color: #6c757d;
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    .stButton>button {
        width: 100%;
    }
    .header-valid {
        background-color: #d4edda;
        color: #155724;
        padding: 8px 12px;
        border-radius: 4px;
        margin: 4px 0;
        font-family: monospace;
        font-weight: 500;
    }
    .header-invalid {
        background-color: #f8d7da;
        color: #721c24;
        padding: 8px 12px;
        border-radius: 4px;
        margin: 4px 0;
        font-family: monospace;
        font-weight: 500;
    }
    .header-extra {
        background-color: #fff3cd;
        color: #856404;
        padding: 8px 12px;
        border-radius: 4px;
        margin: 4px 0;
        font-family: monospace;
        font-weight: 500;
    }
    .info-box {
        background-color: #e7f3ff;
        border-left: 4px solid #2196F3;
        padding: 1rem;
        border-radius: 4px;
        margin: 1rem 0;
    }
    .success-box {
        background-color: #d4edda;
        border-left: 4px solid #28a745;
        padding: 1rem;
        border-radius: 4px;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border-left: 4px solid #ffc107;
        padding: 1rem;
        border-radius: 4px;
        margin: 1rem 0;
    }
    div[data-testid="stMetricValue"] {
        font-size: 1.8rem;
        font-weight: 600;
    }
    .nav-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 12px;
        padding: 8px;
        margin-bottom: 1rem;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
    }
    .nav-button {
        background: transparent;
        border: none;
        color: white;
        padding: 12px 24px;
        margin: 0 4px;
        border-radius: 8px;
        font-weight: 600;
        font-size: 1rem;
        cursor: pointer;
        transition: all 0.3s ease;
        flex: 1;
        text-align: center;
        position: relative;
        overflow: hidden;
    }
    .nav-button:hover {
        background: rgba(255, 255, 255, 0.1);
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
    }
    .nav-button.active {
        background: rgba(255, 255, 255, 0.2);
        box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.1);
        transform: translateY(-1px);
    }
    .nav-button.active::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 3px;
        background: linear-gradient(90deg, #ff6b6b, #4ecdc4);
        border-radius: 0 0 8px 8px;
    }
    .nav-icon {
        margin-right: 8px;
        font-size: 1.1em;
    }
</style>
""", unsafe_allow_html=True)

# FastAPI backend URL
API_URL = "http://16.16.128.44:8000"

# Expected headers for inference (fetch from backend or hardcode)
EXPECTED_HEADERS_INFERENCE = [
    "pl_rade",      # Planet Radius (Earth radii)
    "pl_trandep",   # Transit Depth (%)
    "pl_orbper",    # Orbital Period (days)
    "pl_trandurh",  # Transit Duration (hours)
    "pl_insol",     # Insolation Flux (Earth flux)
    "pl_eqt",       # Equilibrium Temperature (K)
    "st_rad",       # Stellar Radius (Solar radii)
    "st_logg",      # Stellar Surface Gravity (log10(cm/s^2))
    "st_teff",      # Stellar Effective Temperature (K)
    "st_tmag",      # TESS Magnitude
    "st_dist"       # Distance to Star (parsecs)
]

# Expected headers for training (specific features + target variable)
EXPECTED_HEADERS_TRAINING = [
    "pl_orbper",    # Orbital Period (days)
    "pl_trandurh",  # Transit Duration (hours)
    "pl_rade",      # Planet Radius (Earth radii)
    "st_dist",      # Distance to Star (parsecs)
    "st_pmdec",     # Stellar Proper Motion Declination (mas/yr)
    "st_pmra",      # Stellar Proper Motion Right Ascension (mas/yr)
    "dec",          # Declination (degrees)
    "pl_insol",     # Insolation Flux (Earth flux)
    "pl_tranmid",   # Transit Midpoint (BJD)
    "ra",           # Right Ascension (degrees)
    "st_tmag",      # TESS Magnitude
    "pl_trandep",   # Transit Depth (%)
    "pl_eqt",       # Equilibrium Temperature (K)
    "st_rad",       # Stellar Radius (Solar radii)
    "st_logg",      # Stellar Surface Gravity (log10(cm/s^2))
    "st_teff",      # Stellar Effective Temperature (K)
    "exoplanet_status"  # Exoplanet Status (target variable for training)
]

# Backward compatibility
EXPECTED_HEADERS = EXPECTED_HEADERS_INFERENCE

# Header descriptions for tooltips
HEADER_DESCRIPTIONS = {
    "pl_orbper": "Orbital Period (days)",
    "pl_trandurh": "Transit Duration (hours)",
    "pl_rade": "Planet Radius (Earth radii)",
    "st_dist": "Distance to Star (parsecs)",
    "st_pmdec": "Stellar Proper Motion Declination (mas/yr)",
    "st_pmra": "Stellar Proper Motion Right Ascension (mas/yr)",
    "dec": "Declination (degrees)",
    "pl_insol": "Insolation Flux (Earth flux)",
    "pl_tranmid": "Transit Midpoint (BJD)",
    "ra": "Right Ascension (degrees)",
    "st_tmag": "TESS Magnitude",
    "pl_trandep": "Transit Depth (%)",
    "pl_eqt": "Equilibrium Temperature (K)",
    "st_rad": "Stellar Radius (Solar radii)",
    "st_logg": "Stellar Surface Gravity (log10(cm/sÂ²))",
    "st_teff": "Stellar Effective Temperature (K)",
    "exoplanet_status": "Exoplanet Status (CONFIRMED, FALSE POSITIVE, or CANDIDATE)"
}

# ---------------------------
# Helper Functions
# ---------------------------

def fetch_expected_headers(api_url: str) -> List[str]:
    """Fetch expected headers from FastAPI backend"""
    try:
        response = requests.get(f"{api_url}/expected-headers", timeout=5)
        if response.status_code == 200:
            return response.json().get("headers", EXPECTED_HEADERS)
    except:
        pass
    return EXPECTED_HEADERS

def classify_exoplanets(file_content: bytes, filename: str, api_url: str) -> pd.DataFrame:
    """Send CSV to FastAPI backend for classification"""
    try:
        # Ensure API URL doesn't have trailing slash
        api_url = api_url.rstrip('/')

        files = {"csv_file": (filename, file_content, "text/csv")}
        headers = {"Accept": "text/csv"}
        response = requests.post(
            f"{api_url}/inference/classify-csv",
            files=files,
            headers=headers,
            timeout=30
        )

        if response.status_code == 200:
            # Check if response is JSON (error with header info) or CSV
            content_type = response.headers.get('content-type', '')
            if 'application/json' in content_type:
                return response.json()  # Return error dict
            else:
                # Parse CSV response
                df = pd.read_csv(io.StringIO(response.text))
                return df
        else:
            return {"error": f"Classification failed (HTTP {response.status_code}): {response.text}"}
    except requests.exceptions.RequestException as e:
        return {"error": f"Connection error: {str(e)}"}
    except Exception as e:
        return {"error": f"Unexpected error: {str(e)}"}

def train_model(csv_id: int, model_name: str, api_url: str, hyperparameters: Dict = None) -> Dict:
    """Send training request to FastAPI backend"""
    try:
        # Ensure API URL doesn't have trailing slash
        api_url = api_url.rstrip('/')

        # Prepare form data
        data = {
            "model_name": model_name
        }

        if hyperparameters:
            import json
            data["hyperparameters"] = json.dumps(hyperparameters)

        # Send POST request with csv_id as numeric query parameter
        response = requests.post(
            f"{api_url}/model/train",
            params={"csv_id": csv_id},
            data=data,
            headers={"Accept": "application/json"},
            timeout=300  # Training might take longer
        )

        if response.status_code == 200:
            # Parse JSON response with training results
            return response.json()
        else:
            return {"error": f"Training failed (HTTP {response.status_code}): {response.text}"}
    except requests.exceptions.RequestException as e:
        return {"error": f"Connection error: {str(e)}"}
    except Exception as e:
        return {"error": f"Unexpected error: {str(e)}"}

def load_models(api_url: str) -> List[Dict]:
    """Load list of models from FastAPI backend"""
    try:
        # Ensure API URL doesn't have trailing slash
        api_url = api_url.rstrip('/')

        response = requests.get(
            f"{api_url}/inference/get-models",
            timeout=10
        )

        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"Failed to load models (HTTP {response.status_code}): {response.text}"}
    except requests.exceptions.RequestException as e:
        return {"error": f"Connection error: {str(e)}"}
    except Exception as e:
        return {"error": f"Unexpected error: {str(e)}"}

def upload_dataset(file_content: bytes, filename: str, api_url: str) -> Dict:
    """Upload CSV dataset to FastAPI backend"""
    try:
        # Ensure API URL doesn't have trailing slash
        api_url = api_url.rstrip('/')

        files = {"csv_file": (filename, file_content, "text/csv")}
        response = requests.post(
            f"{api_url}/dataset/upload-dataset",
            files=files,
            timeout=30
        )

        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"Upload failed (HTTP {response.status_code}): {response.text}"}
    except requests.exceptions.RequestException as e:
        return {"error": f"Connection error: {str(e)}"}
    except Exception as e:
        return {"error": f"Unexpected error: {str(e)}"}

def load_datasets(api_url: str) -> Dict:
    """Load list of datasets from FastAPI backend"""
    try:
        # Ensure API URL doesn't have trailing slash
        api_url = api_url.rstrip('/')

        response = requests.get(
            f"{api_url}/dataset/get-datasets",
            timeout=10
        )

        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"Failed to load datasets (HTTP {response.status_code}): {response.text}"}
    except requests.exceptions.RequestException as e:
        return {"error": f"Connection error: {str(e)}"}
    except Exception as e:
        return {"error": f"Unexpected error: {str(e)}"}

def display_dataframe_styled(df: pd.DataFrame):
    """Display DataFrame with beautiful styling"""
    # Reorder columns to show label_confidence first if it exists
    if 'label_confidence' in df.columns:
        cols = ['label_confidence'] + [col for col in df.columns if col != 'label_confidence']
        df = df[cols]

    # Apply styling
    def highlight_status(val):
        if val == 'CONFIRMED':
            return 'background-color: #d4edda; color: #155724; font-weight: bold'
        elif val == 'FALSE POSITIVE':
            return 'background-color: #f8d7da; color: #721c24; font-weight: bold'
        elif val == 'CANDIDATE':
            return 'background-color: #fff3cd; color: #856404; font-weight: bold'
        return ''

    # Style the dataframe
    if 'exoplanet_status' in df.columns:
        styled_df = df.style.applymap(
            highlight_status,
            subset=['exoplanet_status']
        ).format(precision=4)
        st.dataframe(styled_df, use_container_width=True, height=400)
    else:
        st.dataframe(df, use_container_width=True, height=400)

def render_editable_headers(df: pd.DataFrame, page_type: str = "inference"):
    """Render editable header interface with color coding"""
    st.markdown("### âï¸ Edit Column Headers")
    st.markdown("**Instructions:** Edit header names to match required format. Valid headers appear in green, invalid in red, extra in yellow.")

    # Select the appropriate expected headers based on page type
    if page_type == "training":
        expected_headers = EXPECTED_HEADERS_TRAINING
    else:
        expected_headers = EXPECTED_HEADERS_INFERENCE

    # Initialize session state for headers if not exists
    if 'edited_headers' not in st.session_state:
        st.session_state.edited_headers = df.columns.tolist()

    # Create columns for better layout
    num_cols = 3
    cols_per_row = st.columns(num_cols)

    edited_headers = []
    header_status = []

    for idx, original_header in enumerate(df.columns):
        col_idx = idx % num_cols
        with cols_per_row[col_idx]:
            # Get current value
            current_value = st.session_state.edited_headers[idx] if idx < len(st.session_state.edited_headers) else original_header

            # Determine header status
            is_valid = current_value in expected_headers

            # Color code the label
            if is_valid:
                label_html = f'<div class="header-valid">â {current_value}</div>'
                status = 'valid'
            elif current_value in edited_headers:
                label_html = f'<div class="header-invalid">â {current_value} (duplicate)</div>'
                status = 'duplicate'
            else:
                # Check if it could be extra
                if original_header not in expected_headers and current_value not in expected_headers:
                    label_html = f'<div class="header-extra">â  {current_value} (extra)</div>'
                    status = 'extra'
                else:
                    label_html = f'<div class="header-invalid">â {current_value}</div>'
                    status = 'invalid'

            st.markdown(label_html, unsafe_allow_html=True)

            # Text input for editing
            new_value = st.text_input(
                f"Column {idx + 1}",
                value=current_value,
                key=f"header_{idx}",
                label_visibility="collapsed",
                help=f"Original: {original_header}"
            )

            edited_headers.append(new_value)
            header_status.append(status)

    # Update session state
    st.session_state.edited_headers = edited_headers

    # Check if all required headers are present
    missing_required = [h for h in expected_headers if h not in edited_headers]
    extra_headers = [h for h in edited_headers if h not in expected_headers]
    duplicate_headers = [h for h in edited_headers if edited_headers.count(h) > 1]

    st.markdown("---")

    # Display status summary
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        valid_count = sum(1 for h in edited_headers if h in expected_headers)
        st.metric("â Valid Headers", f"{valid_count}/{len(expected_headers)}")
    with col2:
        st.metric("â Missing Required", len(missing_required))
    with col3:
        st.metric("â ï¸ Extra Headers", len(extra_headers))
    with col4:
        st.metric("ð Duplicates", len(set(duplicate_headers)))

    # Show details
    if missing_required:
        st.markdown("**â Missing Required Headers:**")
        cols = st.columns(3)
        for idx, header in enumerate(missing_required):
            with cols[idx % 3]:
                st.code(f"{header}\n{HEADER_DESCRIPTIONS.get(header, '')}", language=None)

    # Extra headers are allowed - no need to show warning
    # if extra_headers:
    #     st.markdown("**â ï¸ Extra Headers (will be included but not used for classification):**")
    #     st.info(", ".join(extra_headers))

    if duplicate_headers:
        st.error(f"**ð Duplicate Headers Found:** {', '.join(set(duplicate_headers))}")

    # Return validation status and edited headers
    can_classify = len(missing_required) == 0 and len(duplicate_headers) == 0

    return {
        'can_classify': can_classify,
        'edited_headers': edited_headers,
        'missing': missing_required,
        'extra': extra_headers,
        'duplicates': duplicate_headers
    }

# ---------------------------
# Navigation
# ---------------------------
# Initialize session state for current page
if 'current_page' not in st.session_state:
    st.session_state.current_page = 'Inference'

# Sidebar logo/banner
st.sidebar.markdown("""
<div style="text-align: center; padding: 0.8rem 0 0.5rem 0;">
    <div style="font-size: 2.5rem; margin-bottom: 0.3rem;">
        ðª
    </div>
    <div style="font-size: 1.2rem; font-weight: 700;
                background: linear-gradient(120deg, #667eea 0%, #764ba2 100%);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                margin-bottom: 0.2rem;">
        Exoplanet AI
    </div>
    <div style="font-size: 0.7rem; color: #6c757d; letter-spacing: 1px;">
        CLASSIFICATION SYSTEM
    </div>
</div>
""", unsafe_allow_html=True)

st.sidebar.markdown("---")

# Sidebar navigation
st.sidebar.markdown("## ð§­ Navigation")
st.sidebar.markdown("")  # Add spacing

# Create navigation buttons with custom styling - vertical layout
inference_active = st.session_state.current_page == 'Inference'
if st.sidebar.button(
    "ð® Inference",
    use_container_width=True,
    key="nav_inference_btn",
    type="primary" if inference_active else "secondary"
):
    if not inference_active:
        st.session_state.current_page = 'Inference'
        st.rerun()

training_active = st.session_state.current_page == 'Training'
if st.sidebar.button(
    "ð Training",
    use_container_width=True,
    key="nav_training_btn",
    type="primary" if training_active else "secondary"
):
    if not training_active:
        st.session_state.current_page = 'Training'
        st.rerun()

models_active = st.session_state.current_page == 'Models'
if st.sidebar.button(
    "ð¤ Models",
    use_container_width=True,
    key="nav_models_btn",
    type="primary" if models_active else "secondary"
):
    if not models_active:
        st.session_state.current_page = 'Models'
        st.rerun()

dataset_active = st.session_state.current_page == 'Dataset'
if st.sidebar.button(
    "ð Dataset",
    use_container_width=True,
    key="nav_dataset_btn",
    type="primary" if dataset_active else "secondary"
):
    if not dataset_active:
        st.session_state.current_page = 'Dataset'
        st.rerun()

help_active = st.session_state.current_page == 'Help'
if st.sidebar.button(
    "â Help",
    use_container_width=True,
    key="nav_help_btn",
    type="primary" if help_active else "secondary"
):
    if not help_active:
        st.session_state.current_page = 'Help'
        st.rerun()

st.sidebar.markdown("")  # Add spacing
st.sidebar.markdown("---")

# ---------------------------
# Main UI
# ---------------------------
st.markdown('<h1 class="main-header">ðª Exoplanet Classification System</h1>', unsafe_allow_html=True)
st.markdown('<p class="subtitle">Advanced machine learning classification for exoplanet candidate analysis</p>', unsafe_allow_html=True)

# Sidebar info
st.sidebar.markdown("## ð About This Tool")
st.sidebar.markdown("""
This application uses machine learning to classify exoplanet candidates:

- **â True** - Confirmed exoplanet
- **â False** - Not an exoplanet

Binary classification with confidence scores for each prediction.

Built for astronomical researchers and data scientists.
""")

st.sidebar.markdown("---")

# Show expected headers based on current page
with st.sidebar.expander("ð Required CSV Headers", expanded=False):
    if st.session_state.current_page == 'Training':
        st.markdown("**Training CSV must contain these columns:**")
        headers_to_show = EXPECTED_HEADERS_TRAINING
    else:
        st.markdown("**Inference CSV must contain these columns:**")
        headers_to_show = EXPECTED_HEADERS_INFERENCE
    
    for i, header in enumerate(headers_to_show, 1):
        description = HEADER_DESCRIPTIONS.get(header, "")
        st.markdown(f"**{i}. `{header}`**")
        st.caption(description)

st.sidebar.markdown("---")
st.sidebar.markdown("**ð¡ Tips:**")
st.sidebar.info("""
- Headers are case-sensitive
- Extra columns are allowed
- Edit headers inline if they don't match
""")

# ---------------------------
# Page Content
# ---------------------------
if st.session_state.current_page == 'Inference':
    # ---------------------------
    # Inference Page
    # ---------------------------
    st.markdown("### ð¤ Model Selection")

    # Load models for dropdown
    col1, col2 = st.columns([4, 1])
    with col1:
        # Get available models
        if 'models_list' not in st.session_state:
            st.info("ð¡ Click 'Load Models' to fetch available models from the backend.")
        else:
            models_response = st.session_state['models_list']
            if isinstance(models_response, dict) and 'data' in models_response:
                models_data = models_response['data']
                model_names = [model.get('name', f'Model {idx}') for idx, model in models_data.items()]

                if model_names:
                    selected_model = st.selectbox(
                        "Select Model",
                        options=model_names,
                        help="Choose which model to use for classification"
                    )
                    st.session_state['selected_model'] = selected_model
                else:
                    st.warning("â ï¸ No models available. Please train a model first.")
            else:
                st.warning("â ï¸ Failed to load models. Click 'Load Models' to retry.")

    with col2:
        if st.button("ð Load Models", use_container_width=True, key="inference_load_models"):
            with st.spinner("ð Loading models..."):
                models = load_models(API_URL)
                if isinstance(models, dict) and "error" in models:
                    st.error(f"â {models['error']}")
                else:
                    st.session_state['models_list'] = models
                    st.success("â Models loaded!")
                    st.rerun()
    st.markdown("---")

    st.markdown("### ð Dataset Selection")

    # Load datasets for dropdown
    col1, col2 = st.columns([4, 1])
    with col1:
        # Get available datasets
        if 'datasets_list' not in st.session_state:
            st.info("ð¡ Click 'Load Datasets' to fetch available datasets from the backend.")
        else:
            datasets_response = st.session_state['datasets_list']
            if isinstance(datasets_response, dict) and 'data' in datasets_response:
                datasets_data = datasets_response['data']
                dataset_names = [dataset.get('name', f'Dataset {idx}') for idx, dataset in datasets_data.items()]

                if dataset_names:
                    selected_dataset = st.selectbox(
                        "Select Dataset",
                        options=dataset_names,
                        help="Choose which dataset to use for classification"
                    )
                    st.session_state['selected_dataset'] = selected_dataset
                else:
                    st.warning("â ï¸ No datasets available. Please upload a dataset first.")
            else:
                st.warning("â ï¸ Failed to load datasets. Click 'Load Datasets' to retry.")

    with col2:
        if st.button("ð Load Datasets", use_container_width=True, key="inference_load_datasets"):
            with st.spinner("ð Loading datasets..."):
                datasets = load_datasets(API_URL)
                if isinstance(datasets, dict) and "error" in datasets:
                    st.error(f"â {datasets['error']}")
                else:
                    st.session_state['datasets_list'] = datasets
                    st.success("â Datasets loaded!")
                    st.rerun()

    st.markdown("---")

    # Display dataset data after selection
    if 'selected_dataset' in st.session_state and 'datasets_list' in st.session_state:
        st.markdown("### ð Dataset Preview")

        # Get the selected dataset details
        datasets_response = st.session_state['datasets_list']
        if isinstance(datasets_response, dict) and 'data' in datasets_response:
            datasets_data = datasets_response['data']

            # Find the selected dataset
            selected_dataset_data = None
            for idx, dataset in datasets_data.items():
                if dataset.get('name', '') == st.session_state['selected_dataset']:
                    selected_dataset_data = dataset
                    break

            if selected_dataset_data:
                # Display dataset metrics
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ð Rows", selected_dataset_data.get('rows', 'N/A'))
                with col2:
                    st.metric("ð Columns", selected_dataset_data.get('columns', 'N/A'))
                with col3:
                    st.metric("ð¾ File Size", selected_dataset_data.get('size', 'N/A'))

                # Classify button
                st.markdown("---")
                if 'selected_model' in st.session_state:
                    if st.button("ð Classify Exoplanets", type="primary", use_container_width=True):
                        # Find the numeric IDs for the selected model and dataset
                        model_id = None
                        csv_id = None

                        # Find model_id
                        if 'models_list' in st.session_state:
                            models_data = st.session_state['models_list'].get('data', {})
                            for idx, model in models_data.items():
                                if model.get('name', '') == st.session_state['selected_model']:
                                    model_id = int(idx)
                                    break

                        # Find csv_id
                        for idx, dataset in datasets_data.items():
                            if dataset.get('name', '') == st.session_state['selected_dataset']:
                                csv_id = int(idx)
                                break

                        if model_id is None:
                            st.error("â Could not find model ID")
                        elif csv_id is None:
                            st.error("â Could not find dataset ID")
                        else:
                            with st.spinner("ð Classifying exoplanets... This may take a moment."):
                                # Make API call to classify with model_id and csv_id as query parameters
                                try:
                                    classify_response = requests.post(
                                        f"{API_URL}/inference/classify",
                                        params={
                                            "model_id": str(model_id),
                                            "csv_id": str(csv_id)
                                        },
                                        headers={"Accept": "application/json"},
                                        timeout=60
                                    )

                                    if classify_response.status_code == 200:
                                        # Parse JSON response
                                        result_json = classify_response.json()

                                        # Debug: Show response structure
                                        st.write("**Debug - Response received:**")
                                        st.json(result_json)

                                        # Extract data array and convert to DataFrame
                                        if 'data' in result_json and isinstance(result_json['data'], list):
                                            result_df = pd.DataFrame(result_json['data'])
                                            st.session_state['classified_df'] = result_df
                                            st.session_state['classification_message'] = result_json.get('message', 'Classification complete')
                                            st.success("â Classification complete!")
                                            st.rerun()
                                        else:
                                            st.error("â Unexpected response format from backend")
                                            st.write(f"**Response keys:** {list(result_json.keys())}")
                                            st.write(f"**Has 'data' key:** {'data' in result_json}")
                                            if 'data' in result_json:
                                                st.write(f"**Type of 'data':** {type(result_json['data'])}")
                                                st.write(f"**Value of 'data':** {result_json['data']}")
                                    else:
                                        st.error(f"â Classification failed (HTTP {classify_response.status_code})")
                                        st.code(classify_response.text)
                                except Exception as e:
                                    st.error(f"â Error during classification: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
                else:
                    st.warning("â ï¸ Please select a model first.")
            else:
                st.warning("â ï¸ Selected dataset not found.")
    else:
        st.info("ð¡ Please select both a model and a dataset to proceed.")

    # ---------------------------
    # Display Results
    # ---------------------------
    if 'classified_df' in st.session_state:
        st.markdown("---")
        st.markdown("### ð¯ Classification Results")

        # Show classification message if available
        if 'classification_message' in st.session_state:
            st.info(f"â¹ï¸ {st.session_state['classification_message']}")

        df_result = st.session_state['classified_df']

        # Check if result is a valid DataFrame
        if isinstance(df_result, dict):
            st.error(f"â Unexpected response from backend: {df_result}")
            if st.button("Clear and Try Again"):
                del st.session_state['classified_df']
                st.rerun()
        else:
            # Display summary statistics
            st.markdown("#### ð Classification Summary")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("ð Total Predictions", f"{len(df_result):,}")
            with col2:
                if 'is_exoplanet' in df_result.columns:
                    positive_count = len(df_result[df_result['is_exoplanet'] == 1])
                    pct = (positive_count / len(df_result) * 100) if len(df_result) > 0 else 0
                    st.metric("â Positive (True)", f"{positive_count:,}", f"{pct:.1f}%")
            with col3:
                if 'is_exoplanet' in df_result.columns:
                    negative_count = len(df_result[df_result['is_exoplanet'] == 0])
                    pct = (negative_count / len(df_result) * 100) if len(df_result) > 0 else 0
                    st.metric("â Negative (False)", f"{negative_count:,}", f"{pct:.1f}%")
            with col4:
                if 'label_confidence' in df_result.columns:
                    avg_conf = df_result['label_confidence'].mean()
                    st.metric("ð Avg Confidence", f"{avg_conf:.3f}")

            # Display interactive spreadsheet with conditional formatting
            st.markdown("#### ð Interactive Results Spreadsheet")
            st.markdown(f"**Total columns:** {len(df_result.columns)} | **Rows:** {len(df_result):,}")

            # Reorder columns to show label_confidence and label first
            if 'label_confidence' in df_result.columns and 'is_exoplanet' in df_result.columns:
                # Get all other columns
                other_cols = [col for col in df_result.columns if col not in ['label_confidence', 'is_exoplanet']]
                # Reorder: label_confidence, label, then rest
                df_display = df_result[['label_confidence', 'is_exoplanet'] + other_cols].copy()
            else:
                df_display = df_result.copy()

            # Convert label column to True/False display
            if 'is_exoplanet' in df_display.columns:
                df_display['is_exoplanet'] = df_display['is_exoplanet'].apply(lambda x: 'True' if x == 1 else 'False')

            # Apply conditional formatting: green background for rows where label == 1 (True)
            def highlight_positive_label(row):
                if 'is_exoplanet' in row and row['is_exoplanet'] == 'True':
                    return ['background-color: #d4edda; color: #155724'] * len(row)
                else:
                    return [''] * len(row)

            # Style the dataframe
            styled_df = df_display.style.apply(highlight_positive_label, axis=1).format({
                'label_confidence': '{:.4f}',
                'pl_orbper': '{:.4f}',
                'pl_trandurh': '{:.4f}',
                'pl_rade': '{:.4f}',
                'st_dist': '{:.4f}',
                'st_pmdec': '{:.4f}',
                'st_pmra': '{:.4f}',
                'dec': '{:.6f}',
                'pl_insol': '{:.4f}',
                'pl_tranmid': '{:.5f}',
                'ra': '{:.6f}',
                'st_tmag': '{:.5f}',
                'pl_trandep': '{:.0f}',
                'pl_eqt': '{:.0f}',
                'st_rad': '{:.4f}',
                'st_logg': '{:.4f}',
                'st_teff': '{:.4f}'
            }, na_rep='N/A')

            # Display as interactive dataframe with sorting and filtering
            st.dataframe(
                styled_df,
                use_container_width=True,
                height=600,
                hide_index=True
            )

            col1, col2 = st.columns(2)

            with col1:
                csv = df_result.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="ð¥ Download Results (CSV)",
                    data=csv,
                    file_name="exoplanet_classification_results.csv",
                    mime="text/csv",
                    use_container_width=True
                )

            with col2:
                if st.button("ð Classify New Dataset", use_container_width=True):
                    # Clear session state
                    if 'classified_df' in st.session_state:
                        del st.session_state['classified_df']
                    if 'classification_message' in st.session_state:
                        del st.session_state['classification_message']
                    st.rerun()

elif st.session_state.current_page == 'Training':
    # ---------------------------
    # Training Page
    # ---------------------------
    st.markdown("### ð Model Training")

    # Load datasets for dropdown
    st.markdown("### ð Dataset Selection")

    col1, col2 = st.columns([4, 1])
    with col1:
        # Get available datasets
        if 'datasets_list' not in st.session_state:
            st.info("ð¡ Click 'Load Datasets' to fetch available datasets from the backend.")
        else:
            datasets_response = st.session_state['datasets_list']
            if isinstance(datasets_response, dict) and 'data' in datasets_response:
                datasets_data = datasets_response['data']
                dataset_names = [dataset.get('name', f'Dataset {idx}') for idx, dataset in datasets_data.items()]

                if dataset_names:
                    selected_training_dataset = st.selectbox(
                        "Select Training Dataset",
                        options=dataset_names,
                        help="Choose which dataset to use for training"
                    )
                    st.session_state['selected_training_dataset'] = selected_training_dataset
                else:
                    st.warning("â ï¸ No datasets available. Please upload a dataset first.")
            else:
                st.warning("â ï¸ Failed to load datasets. Click 'Load Datasets' to retry.")

    with col2:
        if st.button("ð Load Datasets", use_container_width=True, key="training_load_datasets"):
            with st.spinner("ð Loading datasets..."):
                datasets = load_datasets(API_URL)
                if isinstance(datasets, dict) and "error" in datasets:
                    st.error(f"â {datasets['error']}")
                else:
                    st.session_state['datasets_list'] = datasets
                    st.success("â Datasets loaded!")
                    st.rerun()

    st.markdown("---")

    # Get dataset ID if selected
    if 'selected_training_dataset' in st.session_state and 'datasets_list' in st.session_state:
        datasets_response = st.session_state['datasets_list']
        if isinstance(datasets_response, dict) and 'data' in datasets_response:
            datasets_data = datasets_response['data']

            # Find the selected dataset ID
            selected_dataset_id = None
            for idx, dataset in datasets_data.items():
                if dataset.get('name', '') == st.session_state['selected_training_dataset']:
                    selected_dataset_id = int(idx)
                    break

            if selected_dataset_id is not None:
                # Hyperparameter configuration section
                st.markdown("### âï¸ Hyperparameter Configuration")

                # Default hyperparameters
                default_hyperparameters = {
                    'lgbm': {
                        'n_estimators': [200, 400],
                        'learning_rate': [0.05, 0.1],
                        'num_leaves': [31, 63],
                        'max_depth': [5, 10],
                        'subsample': [0.8, 1.0],
                        'colsample_bytree': [0.8, 1.0],
                    },
                    'gb': {
                        'n_estimators': [100, 200],
                        'learning_rate': [0.05, 0.1],
                        'max_depth': [3, 5],
                        'subsample': [0.8, 1.0]
                    }
                }

                # Initialize session state for hyperparameters
                import json
                if 'hyperparameters_json' not in st.session_state:
                    st.session_state.hyperparameters_json = json.dumps(default_hyperparameters, indent=2)

                # Expert mode toggle and reset button
                col1, col2 = st.columns([3, 1])
                with col1:
                    expert_mode = st.checkbox("ð¬ Expert Mode (Custom JSON)", value=False, help="Enable to input hyperparameters as JSON")
                with col2:
                    if st.button("ð Reset", help="Reset hyperparameters to default values", use_container_width=True):
                        # Reset to default hyperparameters
                        st.session_state.hyperparameters_json = json.dumps(default_hyperparameters, indent=2)
                        st.success("â Reset to defaults!")
                        st.rerun()

                if expert_mode:
                    # Expert mode: JSON input
                    st.markdown("**ð Edit Hyperparameters as JSON**")
                    st.caption("Modify the JSON below to customize hyperparameters for both LightGBM and Gradient Boosting models.")

                    hyperparameters_json = st.text_area(
                        "Hyperparameters JSON",
                        value=st.session_state.hyperparameters_json,
                        height=300,
                        help="Enter hyperparameters in JSON format",
                        label_visibility="collapsed",
                        key="hyperparams_text_area"
                    )

                    # Update session state
                    st.session_state.hyperparameters_json = hyperparameters_json

                    # Validate JSON
                    try:
                        hyperparameters = json.loads(hyperparameters_json)
                        st.success("â Valid JSON format")
                    except json.JSONDecodeError as e:
                        st.error(f"â Invalid JSON: {str(e)}")
                        hyperparameters = default_hyperparameters
                else:
                    # Default mode: Display locked JSON
                    st.markdown("**ð Default Hyperparameters**")
                    st.caption("Using default hyperparameter configuration. Enable Expert Mode to customize.")

                    st.text_area(
                        "Default Hyperparameters",
                        value=st.session_state.hyperparameters_json,
                        height=300,
                        disabled=True,
                        label_visibility="collapsed"
                    )

                    try:
                        hyperparameters = json.loads(st.session_state.hyperparameters_json)
                    except json.JSONDecodeError:
                        hyperparameters = default_hyperparameters

                st.markdown("---")

                # Model name configuration
                st.markdown("### ð·ï¸ Training Configuration")

                model_name = st.text_input(
                    "Model Name",
                    value="exoplanet_model",
                    help="Give your model a unique name",
                    placeholder="e.g., exoplanet_model_v1"
                )

                st.markdown("---")

                # Training button
                if st.button("ð Start Training", type="primary", use_container_width=True):
                    if not model_name or model_name.strip() == "":
                        st.error("â Please enter a model name")
                    else:
                        # Train model with selected dataset (using numeric ID)
                        try:
                            with st.spinner("ð Training model... This may take several minutes."):
                                result = train_model(selected_dataset_id, model_name.strip(), API_URL, hyperparameters)

                            # Debug: Show what was returned
                            st.write("**Debug - Training Response:**")
                            st.json(result)

                            # Check result
                            if result is None:
                                st.error("â Training returned no result")
                            elif isinstance(result, dict) and "error" in result:
                                st.error(f"â {result['error']}")
                            elif isinstance(result, dict) and "data" in result:
                                # Success case - has "data" key
                                st.session_state['training_result'] = result
                                st.success("â Training complete!")
                                st.rerun()
                            else:
                                st.error(f"â Unexpected response format: {type(result)}")
                                st.write(result)
                        except Exception as e:
                            st.error(f"â Training error: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
            else:
                st.warning("â ï¸ Selected dataset not found.")
    else:
        st.info("ð¡ Please select a training dataset to proceed.")

    # ---------------------------
    # Display Training Results
    # ---------------------------
    if 'training_result' in st.session_state:
        st.markdown("---")
        st.markdown("### ð¯ Training Results")

        training_result = st.session_state['training_result']

        # Check if result is a valid training result
        if isinstance(training_result, dict) and "error" in training_result:
            st.error(f"â Unexpected response from backend: {training_result}")
            if st.button("Clear and Try Again", key="clear_training"):
                del st.session_state['training_result']
                st.rerun()
        else:
            # Display only training metrics
            if isinstance(training_result, dict):
                # Display training data
                if 'data' in training_result and isinstance(training_result['data'], list) and len(training_result['data']) > 0:
                    training_data = training_result['data'][0]
                    
                    st.markdown("#### ð¯ Training Metrics")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        if 'accuracy' in training_data:
                            accuracy_value = training_data['accuracy']
                            st.metric("ð¯ Accuracy", f"{accuracy_value:.3f}")
                    
                    with col2:
                        if 'roc_auc' in training_data:
                            roc_auc_value = training_data['roc_auc']
                            st.metric("ð ROC AUC", f"{roc_auc_value:.3f}")
                    
                    with col3:
                        if 'f1_score' in training_data:
                            f1_value = training_data['f1_score']
                            st.metric("âï¸ F1 Score", f"{f1_value:.3f}")
            
            # Download button for training results
            st.markdown("---")
            col1, col2 = st.columns(2)

            with col1:
                import json
                json_data = json.dumps(training_result, indent=2).encode('utf-8')
                st.download_button(
                    label="ð¥ Download Training Results (JSON)",
                    data=json_data,
                    file_name="training_results.json",
                    mime="application/json",
                    use_container_width=True
                )

            with col2:
                if st.button("ð Train New Model", use_container_width=True, key="new_training"):
                    # Clear session state
                    if 'training_result' in st.session_state:
                        del st.session_state['training_result']
                    if 'edited_headers' in st.session_state:
                        del st.session_state['edited_headers']
                    st.rerun()
    
    else:
        # Show training data requirements when no file is uploaded
        st.markdown("### ð Training Data Requirements")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **Required Columns:**
            - All inference features
            - `exoplanet_status` (target variable)
            """)
        
        with col2:
            st.markdown("""
            **Data Quality:**
            - Minimum 1000 samples
            - Balanced class distribution
            - Clean, validated data
            """)
        
        st.markdown("### ð§ Training Configuration")
        
        with st.expander("ð Model Parameters", expanded=False):
            st.markdown("**Coming Soon:** Advanced training configuration options")
            st.info("This section will include hyperparameter tuning, cross-validation settings, and model architecture options.")

elif st.session_state.current_page == 'Models':
    # ---------------------------
    # Models Page
    # ---------------------------
    st.markdown("### ð¤ Model Management")

    # Refresh button
    col1, col2 = st.columns([4, 1])
    with col1:
        st.markdown("View and manage available models from the backend.")
    with col2:
        if st.button("ð Refresh Models", use_container_width=True):
            if 'models_list' in st.session_state:
                del st.session_state['models_list']
            st.rerun()

    st.markdown("---")

    # Load models button
    if st.button("ð¥ Load Models", type="primary", use_container_width=True):
        with st.spinner("ð Loading models from backend..."):
            models = load_models(API_URL)

            if isinstance(models, dict) and "error" in models:
                st.error(f"â {models['error']}")
            else:
                st.session_state['models_list'] = models
                # Extract model count from the response
                model_count = len(models.get('data', {})) if isinstance(models, dict) and 'data' in models else 0
                st.success(f"â Loaded {model_count} model(s)")
                st.rerun()

    # Display models if loaded
    if 'models_list' in st.session_state:
        st.markdown("---")
        st.markdown("### ð Available Models")

        models_response = st.session_state['models_list']

        # Handle the new JSON format with 'data' key
        if isinstance(models_response, dict) and 'data' in models_response:
            models_data = models_response['data']

            if len(models_data) > 0:
                # Display model count
                st.markdown(f"**Total Models:** {len(models_data)}")

                # Display each model
                for idx, model in models_data.items():
                    with st.expander(f"ð¹ Model {int(idx) + 1}: {model.get('name', 'Unknown')}"):
                        col1, col2 = st.columns(2)

                        with col1:
                            st.markdown("**Model Information:**")
                            st.write(f"**Name:** {model.get('name', 'N/A')}")
                            st.write(f"**Path:** {model.get('path', 'N/A')}")

                        with col2:
                            st.markdown("**Additional Details:**")
                            if 'metrics' in model:
                                metrics = model['metrics']
                                st.write(f"**Accuracy:** {metrics.get('accuracy', 'N/A')}")
                                st.write(f"**F1 Score:** {metrics.get('f1_score', 'N/A')}")
                                st.write(f"**ROC AUC:** {metrics.get('roc_auc', 'N/A')}")
                            else:
                                st.write("No metrics available")

                        # Display full model data as JSON
                        with st.expander("ð View Full Model Data (JSON)"):
                            import json
                            st.json(model)
            else:
                st.info("ð­ No models found in the backend.")
        else:
            st.warning("â ï¸ Unexpected response format from backend.")

elif st.session_state.current_page == 'Dataset':
    # ---------------------------
    # Dataset Page
    # ---------------------------
    st.markdown("### ð Dataset Management")

    # Refresh and Load buttons
    col1, col2 = st.columns([4, 1])
    with col1:
        st.markdown("View, manage, and upload datasets stored on the server.")
    with col2:
        if st.button("ð Refresh Datasets", use_container_width=True):
            if 'datasets_list' in st.session_state:
                del st.session_state['datasets_list']
            st.rerun()

    st.markdown("---")

    # Load datasets button
    if st.button("ð¥ Load Datasets", type="primary", use_container_width=True):
        with st.spinner("ð Loading datasets from server..."):
            datasets = load_datasets(API_URL)

            if isinstance(datasets, dict) and "error" in datasets:
                st.error(f"â {datasets['error']}")
            else:
                st.session_state['datasets_list'] = datasets
                # Extract dataset count from the response
                dataset_count = len(datasets.get('data', {})) if isinstance(datasets, dict) and 'data' in datasets else 0
                st.success(f"â Loaded {dataset_count} dataset(s)")
                st.rerun()

    # Display datasets if loaded
    if 'datasets_list' in st.session_state:
        st.markdown("---")
        st.markdown("### ð Available Datasets")

        datasets_response = st.session_state['datasets_list']

        # Handle the JSON format with 'data' key
        if isinstance(datasets_response, dict) and 'data' in datasets_response:
            datasets_data = datasets_response['data']

            if len(datasets_data) > 0:
                # Display dataset count
                st.markdown(f"**Total Datasets:** {len(datasets_data)}")

                # Create table data
                table_data = []
                for idx, dataset in datasets_data.items():
                    row = {
                        "#": int(idx) + 1,
                        "Dataset Name": dataset.get('name', 'N/A'),
                        "Path": dataset.get('path', 'N/A'),
                        "Size": dataset.get('size', 'N/A'),
                        "Upload Date": dataset.get('upload_date', 'N/A'),
                    }
                    table_data.append(row)

                # Display as DataFrame table
                df_datasets = pd.DataFrame(table_data)
                st.dataframe(df_datasets, use_container_width=True, hide_index=True)

                st.markdown("---")
                st.markdown("### ð Dataset Details")

                # Display each dataset in expandable sections
                for idx, dataset in datasets_data.items():
                    with st.expander(f"ð¹ Dataset {int(idx) + 1}: {dataset.get('name', 'Unknown')}"):
                        col1, col2 = st.columns(2)

                        with col1:
                            st.markdown("**Dataset Information:**")
                            st.write(f"**Name:** {dataset.get('name', 'N/A')}")
                            st.write(f"**Path:** {dataset.get('path', 'N/A')}")
                            st.write(f"**Size:** {dataset.get('size', 'N/A')}")

                        with col2:
                            st.markdown("**Additional Details:**")
                            st.write(f"**Upload Date:** {dataset.get('upload_date', 'N/A')}")
                            st.write(f"**Rows:** {dataset.get('rows', 'N/A')}")
                            st.write(f"**Columns:** {dataset.get('columns', 'N/A')}")

                        # Display full dataset data as JSON
                        with st.expander("ð View Full Dataset Data (JSON)"):
                            import json
                            st.json(dataset)
            else:
                st.info("ð­ No datasets found on the server.")
        else:
            st.warning("â ï¸ Unexpected response format from backend.")

    st.markdown("---")
    st.markdown("### ð¤ Upload New Dataset")

    # File uploader
    uploaded_dataset = st.file_uploader(
        "Choose a CSV file to upload to the server",
        type=["csv"],
        help="Upload a CSV file to be stored on the server",
        key="dataset_uploader"
    )

    if uploaded_dataset is not None:
        # Read file content
        dataset_content = uploaded_dataset.read()

        # Preview uploaded file
        st.markdown("### ð Dataset Preview")
        try:
            df_dataset = pd.read_csv(io.BytesIO(dataset_content))

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ð Rows", f"{len(df_dataset):,}")
            with col2:
                st.metric("ð Columns", len(df_dataset.columns))
            with col3:
                st.metric("ð¾ File Size", f"{len(dataset_content) / 1024:.1f} KB")

            # Show preview
            with st.expander("ð View Dataset Sample", expanded=True):
                st.dataframe(df_dataset.head(10), use_container_width=True)

            st.markdown("---")

            # Upload button
            if st.button("ð¤ Upload to Server", type="primary", use_container_width=True):
                with st.spinner("ð¤ Uploading dataset to server..."):
                    result = upload_dataset(dataset_content, uploaded_dataset.name, API_URL)

                    if isinstance(result, dict) and "error" in result:
                        st.error(f"â {result['error']}")
                    else:
                        st.success(f"â Dataset uploaded successfully!")
                        if isinstance(result, dict):
                            st.json(result)

        except Exception as e:
            st.error(f"â Error reading CSV file: {str(e)}")
            st.info("ð¡ Please ensure your file is a valid CSV format.")
    else:
        # Show upload instructions when no file is uploaded
        st.markdown("### ð Upload Instructions")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **What you can upload:**
            - Training datasets with labels
            - Inference datasets
            - Any CSV file for later use
            """)

        with col2:
            st.markdown("""
            **Benefits:**
            - Store datasets on the server
            - Reuse datasets across sessions
            - Share datasets with team members
            """)

        st.markdown("---")
        st.info("ð¡ **Tip:** Upload your CSV file using the file uploader above. The file will be saved on the server and can be accessed later for training or inference.")

elif st.session_state.current_page == 'Help':
    # ---------------------------
    # Help Page
    # ---------------------------
    st.markdown("### â Help & Documentation")
    st.markdown("Comprehensive guide to using the Exoplanet Classification System")

    st.markdown("---")

    # Table of Contents
    st.markdown("## ð Table of Contents")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        - [Machine Learning Models](#machine-learning-models)
        - [Classification Categories](#classification-categories)
        - [Using the System](#using-the-system)
        """)
    with col2:
        st.markdown("""
        - [Data Requirements](#data-requirements)
        - [API Information](#api-information)
        - [Troubleshooting](#troubleshooting)
        """)

    st.markdown("---")

    # Machine Learning Models
    st.markdown("## ð¤ Machine Learning Models")
    st.markdown("Our system uses an ensemble of gradient boosting models for accurate exoplanet classification.")

    with st.expander("ð **LightGBM (Light Gradient Boosting Machine)**", expanded=True):
        st.markdown("""
        **Description:**
        LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It's designed for distributed and efficient training.

        **Key Features:**
        - **Faster training speed** and higher efficiency
        - **Lower memory usage**
        - Better accuracy with large datasets
        - Support for parallel and GPU learning

        **Default Hyperparameters:**
        - `n_estimators`: [200, 400] - Number of boosting iterations
        - `learning_rate`: [0.05, 0.1] - Step size shrinkage
        - `num_leaves`: [31, 63] - Maximum tree leaves
        - `max_depth`: [5, 10] - Maximum tree depth
        - `subsample`: [0.8, 1.0] - Training data sampling ratio
        - `colsample_bytree`: [0.8, 1.0] - Feature sampling ratio

        **Best For:** Large datasets with complex patterns
        """)

    with st.expander("ð² **Gradient Boosting Classifier**"):
        st.markdown("""
        **Description:**
        Gradient Boosting builds an ensemble of weak prediction models (typically decision trees) in a stage-wise fashion.

        **Key Features:**
        - **High predictive accuracy**
        - Handles both numerical and categorical features
        - Robust to outliers
        - Provides feature importance rankings

        **Default Hyperparameters:**
        - `n_estimators`: [100, 200] - Number of boosting stages
        - `learning_rate`: [0.05, 0.1] - Learning rate
        - `max_depth`: [3, 5] - Maximum tree depth
        - `subsample`: [0.8, 1.0] - Training data sampling ratio

        **Best For:** Medium-sized datasets with mixed feature types
        """)

    with st.expander("ð **Model Ensemble & Stacking**"):
        st.markdown("""
        **Description:**
        Our system combines multiple models using a stacking ensemble approach for optimal performance.

        **How it Works:**
        1. **Base Models** (Level 0): LightGBM and Gradient Boosting independently make predictions
        2. **Meta Model** (Level 1): Combines base model predictions using a meta-learner
        3. **Final Prediction**: Weighted average or voting mechanism produces final classification

        **Benefits:**
        - **Higher accuracy** than individual models
        - **Reduced overfitting** through model diversity
        - **Robust predictions** across different data distributions
        """)

    st.markdown("---")

    # Classification Categories
    st.markdown("## ð·ï¸ Classification Categories")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        <div class="success-box">
            <h4>â CONFIRMED</h4>
            <p><strong>Definition:</strong> Verified exoplanet with strong evidence</p>
            <p><strong>Confidence:</strong> >85%</p>
            <p><strong>Characteristics:</strong></p>
            <ul>
                <li>Clear transit signal</li>
                <li>Consistent orbital period</li>
                <li>Ruled out false positives</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="warning-box">
            <h4>â FALSE POSITIVE</h4>
            <p><strong>Definition:</strong> Not an exoplanet</p>
            <p><strong>Confidence:</strong> >80%</p>
            <p><strong>Common Causes:</strong></p>
            <ul>
                <li>Binary star systems</li>
                <li>Stellar variability</li>
                <li>Instrumental artifacts</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown("""
        <div class="info-box">
            <h4>ð CANDIDATE</h4>
            <p><strong>Definition:</strong> Requires additional data</p>
            <p><strong>Confidence:</strong> 50-85%</p>
            <p><strong>Next Steps:</strong></p>
            <ul>
                <li>Additional observations</li>
                <li>Follow-up analysis</li>
                <li>Radial velocity confirmation</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # Using the System
    st.markdown("## ð Using the System")

    with st.expander("**1ï¸â£ Inference Workflow**", expanded=True):
        st.markdown("""
        **Step 1: Load Models**
        Click "Load Models" to fetch available models from the backend

        **Step 2: Select Model**
        Choose the model you want to use for classification from the dropdown

        **Step 3: Upload Data**
        Upload a CSV file with exoplanet candidate measurements

        **Step 4: Validate Headers**
        Edit column names if needed to match required format

        **Step 5: Classify**
        Click "Classify Exoplanets" to get predictions

        **Step 6: Download Results**
        Download the classification results as CSV
        """)

    with st.expander("**2ï¸â£ Training Workflow**"):
        st.markdown("""
        **Step 1: Prepare Training Data**
        Ensure your CSV includes all required features + `exoplanet_status` label

        **Step 2: Upload Training CSV**
        Upload your labeled training dataset

        **Step 3: Configure Hyperparameters**
        Use default hyperparameters or enable Expert Mode for custom JSON

        **Step 4: Start Training**
        Click "Start Training" and wait for the process to complete

        **Step 5: Review Metrics**
        Examine accuracy, ROC AUC, and F1 score

        **Step 6: Download Results**
        Save training results as JSON for future reference
        """)

    st.markdown("---")

    # Data Requirements
    st.markdown("## ð Data Requirements")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### Inference Features (11)")
        for i, header in enumerate(EXPECTED_HEADERS_INFERENCE, 1):
            st.markdown(f"**{i}. `{header}`**")
            st.caption(HEADER_DESCRIPTIONS.get(header, ""))

    with col2:
        st.markdown("### Training Features (17)")
        st.info("All inference features plus additional parameters and target variable")
        st.markdown("""
        **Additional Required:**
        - `st_pmdec`, `st_pmra` (Proper motion)
        - `dec`, `ra` (Coordinates)
        - `pl_tranmid` (Transit midpoint)
        - `exoplanet_status` (Target label)
        """)

    st.markdown("---")

    # API Information
    st.markdown("## ð API Information")

    with st.expander("**API Endpoints**"):
        st.markdown(f"""
        **Backend URL:** `{API_URL}`

        **Available Endpoints:**
        - `POST /inference/classify-csv` - Classify exoplanet candidates
        - `GET /inference/get-models` - List available models
        - `POST /model/train` - Train new model
        - `POST /dataset/upload` - Upload dataset to server

        **Request Format:**
        Multipart form data with CSV file attachment

        **Response Format:**
        - Inference: CSV file with predictions
        - Training: JSON with metrics and model info
        """)

    st.markdown("---")

    # Troubleshooting
    st.markdown("## ð§ Troubleshooting")

    with st.expander("**â Common Issues & Solutions**"):
        st.markdown("""
        **Problem:** "Missing Required Headers" error
        **Solution:** Use the header editor to rename columns to match expected format

        ---

        **Problem:** "Connection Error" message
        **Solution:** Check that the backend server is running and accessible

        ---

        **Problem:** Model training fails
        **Solution:** Ensure dataset has minimum 1000 samples and includes `exoplanet_status` column

        ---

        **Problem:** Invalid JSON in Expert Mode
        **Solution:** Verify JSON syntax - use JSON validator or reset to defaults

        ---

        **Problem:** Low classification confidence
        **Solution:** Consider:
        - Using more training data
        - Adjusting hyperparameters
        - Checking data quality
        """)

    st.markdown("---")

    # Contact & Support
    st.markdown("## ð¬ Contact & Support")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        **For Technical Support:**
        - Check the troubleshooting section above
        - Review the API documentation
        - Verify data format requirements
        """)

    with col2:
        st.markdown("""
        **System Information:**
        - **Backend:** FastAPI
        - **Frontend:** Streamlit
        - **Models:** LightGBM + Gradient Boosting
        - **Version:** 1.0.0
        """)

# ---------------------------
# Footer
# ---------------------------
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #6c757d; padding: 2rem 0;'>
        <p style='margin: 0; font-size: 0.9rem;'>
            <strong>Exoplanet Classification System</strong> | Built with Streamlit + FastAPI
        </p>
        <p style='margin: 0.5rem 0 0 0; font-size: 0.8rem;'>
            For research and educational purposes
        </p>
    </div>
    """,
    unsafe_allow_html=True
)